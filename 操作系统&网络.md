计算机/操作系统/网络协议/理论

1. 对nio有了解吗

2. 怎么保证幂等性

3. 为什么用docker，好处是什么

   ```
   1. 一、环境一致性 & 跨平台
   二、资源高效 & 快速启动
   三、隔离性与安全性
   ```

   

4. 你在项目中怎么用的docker

5. 怎么做的限流

6. 怎么防止超卖

7. 怎么保证幂等性

8. 单例模式的创建；





```
tcp/ip 网络模型
TCP 三次握手和四次挥手 传输层
Tcp/ udp  传输层
ARP 协议详解(网络层)
NAT 协议详解（网络层）
```



以下是一个在 Linux 中使用 `epoll` 处理多个 Socket 连接的完整示例代码，包含连接建立、数据读取、回写响应等功能。代码包含详细注释，适用于高并发场景。

------

### **C 语言代码实现**

```c
#include <stdio.h>
#include <stdlib.h>
#include <string.h>
#include <unistd.h>
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>
#include <sys/epoll.h>
#include <fcntl.h>
#include <errno.h>

#define MAX_EVENTS 1024
#define BUFFER_SIZE 4096
#define SERVER_PORT 8080

// 设置文件描述符为非阻塞模式
static void set_nonblocking(int fd) {
    int flags = fcntl(fd, F_GETFL, 0);
    fcntl(fd, F_SETFL, flags | O_NONBLOCK);
}

// 处理新连接
static void handle_accept(int epoll_fd, int listen_fd) {
    struct sockaddr_in client_addr;
    socklen_t client_len = sizeof(client_addr);
    int client_fd = accept(listen_fd, (struct sockaddr*)&client_addr, &client_len);
    if (client_fd == -1) {
        perror("accept");
        return;
    }

    // 设置为非阻塞模式
    set_nonblocking(client_fd);

    // 注册到 epoll，监听读事件（边缘触发模式）
    struct epoll_event ev;
    ev.events = EPOLLIN | EPOLLET; // 边缘触发（Edge Triggered）
    ev.data.fd = client_fd;
    if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, client_fd, &ev) == -1) {
        perror("epoll_ctl: client_fd");
        close(client_fd);
    }

    printf("New connection: %s:%d (fd=%d)\n",
           inet_ntoa(client_addr.sin_addr), ntohs(client_addr.sin_port), client_fd);
}

// 处理客户端数据读取和回写
static void handle_client(int epoll_fd, int client_fd) {
    char buffer[BUFFER_SIZE];
    ssize_t n;

    // 循环读取数据（边缘触发必须读完）
    while ((n = read(client_fd, buffer, BUFFER_SIZE)) > 0) {
        printf("Received %zd bytes from fd=%d\n", n, client_fd);

        // 示例：回显数据（Echo）
        if (write(client_fd, buffer, n) != n) {
            perror("write");
            break;
        }
    }

    // 处理连接关闭或错误
    if (n == 0 || (n == -1 && errno != EAGAIN)) {
        printf("Connection closed: fd=%d\n", client_fd);
        close(client_fd); // 关闭时会自动从 epoll 移除
    } else if (n == -1) {
        // EAGAIN 表示数据已读完
    }
}

int main() {
    int listen_fd, epoll_fd;
    struct sockaddr_in server_addr;
    struct epoll_event events[MAX_EVENTS];

    // 创建监听 Socket
    listen_fd = socket(AF_INET, SOCK_STREAM, 0);
    if (listen_fd == -1) {
        perror("socket");
        exit(EXIT_FAILURE);
    }

    // 设置 SO_REUSEADDR 避免 TIME_WAIT
    int opt = 1;
    setsockopt(listen_fd, SOL_SOCKET, SO_REUSEADDR, &opt, sizeof(opt));

    // 绑定地址和端口
    memset(&server_addr, 0, sizeof(server_addr));
    server_addr.sin_family = AF_INET;
    server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
    server_addr.sin_port = htons(SERVER_PORT);
    if (bind(listen_fd, (struct sockaddr*)&server_addr, sizeof(server_addr)) == -1) {
        perror("bind");
        close(listen_fd);
        exit(EXIT_FAILURE);
    }

    // 开始监听
    if (listen(listen_fd, SOMAXCONN) == -1) {
        perror("listen");
        close(listen_fd);
        exit(EXIT_FAILURE);
    }

    printf("Server listening on port %d\n", SERVER_PORT);

    // 创建 epoll 实例
    epoll_fd = epoll_create1(0);
    if (epoll_fd == -1) {
        perror("epoll_create1");
        close(listen_fd);
        exit(EXIT_FAILURE);
    }

    // 注册监听 Socket 到 epoll
    struct epoll_event ev;
    ev.events = EPOLLIN | EPOLLET; // 边缘触发模式
    ev.data.fd = listen_fd;
    if (epoll_ctl(epoll_fd, EPOLL_CTL_ADD, listen_fd, &ev) == -1) {
        perror("epoll_ctl: listen_fd");
        close(listen_fd);
        close(epoll_fd);
        exit(EXIT_FAILURE);
    }

    // 事件循环
    while (1) {
        int n_ready = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);
        if (n_ready == -1) {
            perror("epoll_wait");
            break;
        }

        for (int i = 0; i < n_ready; i++) {
            int fd = events[i].data.fd;

            // 处理新连接
            if (fd == listen_fd) {
                handle_accept(epoll_fd, listen_fd);
            }
            // 处理客户端数据
            else if (events[i].events & EPOLLIN) {
                handle_client(epoll_fd, fd);
            }
        }
    }

    // 清理
    close(listen_fd);
    close(epoll_fd);
    return 0;
}
```

------

### **代码说明**

#### **1. 核心步骤**

1. **创建监听 Socket**：绑定端口并开始监听。

2. **设置非阻塞模式**：通过 `fcntl` 设置 Socket 为非阻塞。

3. **创建 epoll 实例**：使用 `epoll_create1`。

4. **注册监听 Socket**：监听 `EPOLLIN` 事件（新连接）。

5. **事件循环**：通过 `epoll_wait` 等待事件就绪。

6. 处理事件

   ：

   - **新连接**：调用 `accept`，将客户端 Socket 注册到 epoll。
   - **客户端数据**：读取数据并回写（Echo）。

#### **2. 边缘触发（Edge Triggered, ET）模式**

- 使用 `EPOLLET` 标志，确保事件仅在状态变化时触发。
- **必须一次性读取所有数据**：在 `handle_client` 中使用循环读取，直到返回 `EAGAIN`。

#### **3. 关键函数**

- `epoll_ctl`：注册/修改/删除事件。
- `epoll_wait`：等待事件就绪。
- `accept`/`read`/`write`：处理连接和数据。

------

### **编译与运行**

1. **编译代码**：

   ```bash
   gcc epoll_server.c -o epoll_server
   ```

2. **启动服务器**：

   ```bash
   ./epoll_server
   ```

3. **测试连接**：

   ```bash
   telnet 127.0.0.1 8080
   ```

   输入任意字符，服务器会回显相同内容。

------

### **扩展功能**

1. **错误处理**：添加更完善的错误恢复逻辑。
2. **协议解析**：在 `handle_client` 中解析 HTTP 或其他协议。
3. **写事件处理**：如果 `write` 返回 `EAGAIN`，监听 `EPOLLOUT` 事件继续发送。
4. **多线程/进程**：将就绪事件分发给工作线程处理。



## NIO

### **.核心组件**

#### **1.1 Buffer（缓冲区）**

- **作用**：NIO 中所有数据的读写都通过缓冲区完成。

- **类型**：`ByteBuffer`, `CharBuffer`, `IntBuffer` 等（支持基本数据类型）。

- 关键属性：

  - `capacity`：缓冲区容量。
  - `position`：当前读写位置。
  - `limit`：可操作的最大位置。
  - `mark`：标记位置，用于 `reset()`。

- 常用方法：

  ```java
  ByteBuffer buffer = ByteBuffer.allocate(1024); // 分配堆内存
  ByteBuffer directBuffer = ByteBuffer.allocateDirect(1024); // 分配直接内存（零拷贝优化）
  buffer.put(data);  // 写入数据
  buffer.flip();     // 切换为读模式
  buffer.clear();    // 清空缓冲区（重置 position 和 limit）
  ```

#### **1.2 Channel（通道）**

- **作用**：连接数据源（如文件、网络套接字），支持双向读写。
- 常见实现：
  - `FileChannel`：文件操作。
  - `SocketChannel`、`ServerSocketChannel`：TCP 网络通信。
  - `DatagramChannel`：UDP 网络通信。
- **特点**：通过 Channel 与 Buffer 交互，支持非阻塞模式。

#### **1.3 Selector（选择器）**

- **作用**：多路复用器，监听多个 Channel 的事件（如连接、读、写）。
- 核心机制：
  - 将多个 Channel 注册到 Selector，通过 `select()` 监听就绪事件。
  - 基于操作系统底层的 `epoll`（Linux）、`kqueue`（Mac）或 `select`（Windows）实现。
- 事件类型：
  - `SelectionKey.OP_ACCEPT`：接受连接。
  - `SelectionKey.OP_CONNECT`：连接完成。
  - `SelectionKey.OP_READ`：数据可读。
  - `SelectionKey.OP_WRITE`：数据可写。

#### **I/O 多路复用（I/O Multiplexing）**

- **行为**：通过 `select`/`poll`/`epoll` 监听多个文件描述符（fd），当某个 fd 就绪时返回。
- **对应 Java**：NIO 的 `Selector`（底层使用 `epoll`）。

```c++
// Linux epoll 示例（简化）
int epoll_fd = epoll_create1(0);
struct epoll_event event;
event.events = EPOLLIN; // 监听读事件
event.data.fd = socket_fd;
epoll_ctl(epoll_fd, EPOLL_CTL_ADD, socket_fd, &event);

while (1) {
    int n = epoll_wait(epoll_fd, events, MAX_EVENTS, -1);
    for (int i = 0; i < n; i++) {
        if (events[i].data.fd == socket_fd) {
            // 处理就绪的 fd
        }
    }
}
```

### **4. 关键差异与优化**

#### **(1) 零拷贝（Zero-Copy）**

- **Linux**：通过 `sendfile()` 或 `splice()` 实现文件到网络的直接传输，减少数据拷贝。

- Java：通过FileChannel.transferTo()，调用sendfile()。

  ```java
  FileChannel sourceChannel = new FileInputStream("data.txt").getChannel();
  FileChannel destChannel = new FileOutputStream("dest.txt").getChannel();
  sourceChannel.transferTo(0, sourceChannel.size(), destChannel); // 零拷贝
  ```

#### **直接内存（Direct Buffer）**

- **Java NIO**：使用 `ByteBuffer.allocateDirect()` 分配堆外内存，避免 JVM 堆与 Native 堆的数据拷贝。
- **底层机制**：直接操作物理内存，提升 I/O 效率。

#### **(3) 多路复用的实现**

- Linux `epoll` 的优势：
  - 事件驱动，无需遍历所有 fd。
  - 支持水平触发（LT）和边缘触发（ET）模式。
- **Java NIO**：默认使用水平触发，确保数据完全处理。



### **四、Java NIO 中的事件映射**

在 Java NIO 的 `Selector` 中，事件通过 `SelectionKey` 定义：

| **Linux `epoll` 事件** | **Java NIO `SelectionKey` 事件** |        **说明**        |
| :--------------------: | :------------------------------: | :--------------------: |
|       `EPOLLIN`        |            `OP_READ`             |  数据可读或新连接到达  |
|       `EPOLLOUT`       |            `OP_WRITE`            |        数据可写        |
|   `EPOLLCONN` (隐式)   |           `OP_CONNECT`           | 连接完成（成功或失败） |
|       `EPOLLERR`       |      通过 `isValid()` 检查       |        错误发生        |
|      `EPOLLRDHUP`      |    `read()` 返回 `-1` 或 `0`     |      对端关闭连接      |





## 一，内存碎片带来的问题

### 1.**内存分配效率降低**

- **碎片化导致分配延迟**：当内存中存在大量不连续的小块空闲内存时，内存分配器（如jemalloc）需要花费更多时间寻找或合并足够大的连续内存块来满足新的分配请求。尤其是在需要分配较大内存块时，分配器可能需要多次尝试或进行碎片整理，增加了延迟。
- **频繁的系统调用**：高碎片率可能导致分配器更频繁地向操作系统申请新内存或释放内存，引发更多的系统调用，增加CPU开销。

### 2.**内存利用率下降**

- **实际可用内存减少**：虽然总空闲内存可能足够，但由于碎片的存在，这些内存分散为无法被利用的小块。例如，即使有1GB的空闲内存，若全为1MB的碎片，Redis无法分配一个2MB的对象，导致内存浪费。
- **触发Swap或OOM**：当物理内存不足时，系统可能将部分内存页交换到磁盘（Swap），大幅降低性能；极端情况下可能触发OOM Killer终止进程。

### 3. **缓存局部性恶化**

- **数据分散存储**：内存碎片使得Redis数据在物理内存中分散存放，破坏了空间局部性。CPU缓存的命中率降低，访问数据时需更多次访问内存，导致延迟上升。
- **TLB（页表缓冲）压力增大**：分散的内存页增加TLB未命中率，进一步拖慢内存访问速度。



### 二，什么是幂等性？

**幂等性（Idempotence）** 是计算机领域的一个重要概念，表示 **对同一个操作的多次执行，结果与一次执行完全相同**。简单来说：

- **无论操作执行多少次，系统的最终状态都一致**。
- 即使因网络重试、消息重复等原因导致操作被多次触发，也不会产生副作用（如重复扣款、重复创建订单等）。

------

#### 为什么需要幂等性？

在分布式系统、网络通信、消息队列等场景中，**重复请求不可避免**，例如：

1. 网络抖动导致HTTP请求自动重试。
2. RabbitMQ消费者处理消息后ACK未送达，消息被重复投递。
3. 用户误操作多次点击提交按钮。
   **幂等性设计可以保证系统在重复请求下的数据一致性**，避免业务逻辑错误。

------

#### 幂等性的典型场景

|     **场景**     |        **非幂等操作**         |                     **幂等设计**                      |
| :--------------: | :---------------------------: | :---------------------------------------------------: |
|   **支付系统**   |   重复扣款导致用户余额错误    |       通过订单唯一ID保证同一笔支付仅扣款一次。        |
|   **订单创建**   |   用户重复提交生成多个订单    | 提交时生成唯一令牌（Token），后端校验令牌是否已使用。 |
| **消息队列消费** | 消息重复消费导致数据重复处理  |        消费者记录已处理消息ID，避免重复执行。         |
|  **数据库更新**  | `UPDATE` 操作覆盖其他并发修改 |       使用乐观锁（`version`字段）控制更新条件。       |

------

#### 如何实现幂等性？

##### 1. **唯一标识（ID）**

- 为每个操作分配全局唯一ID（如UUID、雪花算法ID），通过数据库唯一索引或缓存（Redis）去重。

  ```
  CREATE TABLE orders (
      id VARCHAR(32) PRIMARY KEY,  -- 唯一订单ID
      amount DECIMAL,
      status INT
  );
  ```

##### 2. **乐观锁（Optimistic Lock）**

- 在数据更新时，通过版本号（version）或时间戳控制并发。

  ```
  UPDATE account 
  SET balance = balance - 100, version = version + 1 
  WHERE id = 123 AND version = 5;  -- 仅当版本匹配时更新
  ```

##### 3. **状态机（State Machine）**

- 限制操作只能在特定状态下执行（如“已支付”的订单不允许再次支付）。

  ```
  if (order.getStatus() != PAID) {
      processPayment(order);
      order.setStatus(PAID);
  }
  ```

##### 4. **Token 机制**

- 客户端提交请求前先获取唯一Token，服务端校验Token有效性后执行操作。

  ```
  // 生成Token并存到Redis（有效期5分钟）
  String token = UUID.randomUUID().toString();
  redis.setex("req_token:" + userId, 300, token);
  
  // 客户端提交Token，服务端校验
  if (redis.del("req_token:" + userId, token) == 1) {
      processRequest();
  }
  ```

##### 5. **消息队列去重**

- RabbitMQ消费者记录已处理消息的ID（如存入Redis），后续消费前先检查是否已处理。

  ```
  if redis.get(msg_id) is None:
      process_message(msg)
      redis.set(msg_id, "processed", ex=3600)
  ```

------

#### 幂等性 vs 事务

|   **特性**   |         **幂等性**          |              **事务**              |
| :----------: | :-------------------------: | :--------------------------------: |
|   **目标**   |      解决重复操作问题       |      保证操作的原子性、一致性      |
| **适用场景** | 网络请求、消息队列、API设计 |      数据库复杂操作（如转账）      |
| **实现方式** |  唯一ID、版本号、状态机等   | ACID（数据库事务、分布式事务框架） |
| **关键区别** |    关注多次操作的副作用     |        关注单次操作的完整性        |

------

#### 注意事项

1. **幂等性需结合业务设计**：并非所有操作都需要幂等（如日志记录）。
2. **唯一ID的生成必须可靠**：避免ID冲突导致去重失效。
3. **缓存和数据库的一致性**：如用Redis记录已处理ID，需考虑缓存穿透、雪崩等问题。

------

**总结**：
幂等性是分布式系统容错设计的核心思想之一，尤其在消息队列（如RabbitMQ）、支付、订单等场景中，能有效避免重复操作导致的数据错误。实现关键是 **通过唯一标识、状态控制或版本机制，将“多次请求”转换为“一次有效操作”**。